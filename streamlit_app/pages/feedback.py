import streamlit as st
from ..utils import load_resources
from ..inference import (
    predict_sorted,
    compute_hierarchy_probs,
    THRESHOLD_DEFAULT,
    section_dict, division_dict,
    group_dict, label_dict_inv
)
import time

# Page sidebar description
description = (
    "Help us improve the classifier by providing real-world feedback. "
    "<br><br>On this page, you'll review a sample prediction and tell us whether the model got it right. "
    "<br>You can also leave comments to highlight errors or edge cases."
    "<br><br>ğŸ“Œ Your input helps us evaluate and improve system performance over time."
)

def run():
    """
    Render the Feedback page of the Streamlit app.
    Get the User feedback on labels and store them in database for evaluations
    """
    
    st.title("ğŸ§¾ Feedback")
    st.markdown(
        "Help us improve the classifier by evaluating predictions on real data."
    )


    # This runs only once per session and sets up our "memory".
    if 'sample_fetched' not in st.session_state:
        st.session_state.sample_fetched = False
        st.session_state.text = ""
        st.session_state.predictions = []
        
    # Load cached model/tokenizer
    tokenizer, model = load_resources()
    
    if st.button("ğŸ”„ Fetch a New Sample for Review", use_container_width=True):
        # The button's only job is to get data and save it to our "memory".
        st.session_state.text_id = 12345
        st.session_state.text = 'Ø§Ø±Ø§ÛŒÙ‡ Ú©Ù„ÛŒÙ‡ Ø®Ø¯Ù…Ø§Øª Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ Ø´Ø§Ù…Ù„ Ù…Ø´Ø§ÙˆØ±Ù‡ Ø§Ø¬Ø±Ø§ Ù†Ø¸Ø§Ø±Øª Ø¨Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù†ÙˆØ§Ø¹ Ø·Ø±Ø­Ù‡Ø§ÛŒ Ø§Ø¨ÛŒØ§Ø±ÛŒ Ù‚Ø·Ø±Ù‡ Ø§ÛŒ Ø¨Ø§Ø±Ø§Ù†ÛŒ ØªØ­Øª ÙØ´Ø§Ø± Ùˆ Ø¨Ø±Ù‚ÛŒ Ú©Ø±Ø¯Ù† Ú†Ø§Ù‡ Ù‡Ø§ÛŒ Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ Ø¬Ù‡Øª Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„ÛŒØ§Øª Ø²Ø±Ø§Ø¹ÛŒ Ø¯Ø± Ù…Ø²Ø§Ø±Ø¹ Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ Ùˆ Ø¨Ø§ØºØ§Øª Ùˆ ØªØ³Ø·ÛŒØ­ Ùˆ Ø§Ù…Ø§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ùˆ Ø§Ø¨Ø®ÛŒØ²Ø¯Ø§Ø±ÛŒ Ùˆ Ø²Ù‡Ú©Ø´ÛŒ Ú©Ù„ÛŒÙ‡ Ø²Ù…ÛŒÙ†Ù‡Ø§ÛŒ Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ Ø¨Ø§ØºØ¯Ø§Ø±ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ÙØ¶Ø§ÛŒ Ø³Ø¨Ø² Ú¯Ù„Ø®Ø§Ù†Ù‡ Ù‡Ø§ÛŒ ÙØ¶Ø§ÛŒ Ø§Ø²Ø§Ø¯ ØªÙ‡ÛŒÙ‡ ØªÙˆÙ„ÛŒØ¯ ØªÚ©Ø«ÛŒØ± Ùˆ Ù¾Ø±ÙˆØ±Ø´ Ú¯Ù„Ù‡Ø§ÛŒ Ø§Ù¾Ø§Ø±ØªÙ…Ø§Ù†ÛŒ Ùˆ Ø§Ù†ÙˆØ§Ø¹ Ù†Ù‡Ø§Ù„ ØªÙ‡ÛŒÙ‡ ØªÙˆÙ„ÛŒØ¯ Ø®Ø±ÛŒØ¯ ÙØ±ÙˆØ´ Ø¨Ø³ØªÙ‡ Ø¨Ù†Ø¯ÛŒ ÙˆØ§Ø±Ø¯Ø§Øª ØµØ§Ø¯Ø±Ø§Øª Ø§Ù†ÙˆØ§Ø¹ Ú©ÙˆØ¯Ù‡Ø§ÛŒ Ø´ÛŒÙ…ÛŒØ§ÛŒÛŒ Ø§Ù†ÙˆØ§Ø¹ Ø¨Ø°Ø± Ùˆ Ù†Ø´Ø§ Ùˆ Ú©Ù…Ù¾ÙˆØ³Øª Ùˆ Ø³Ù…ÙˆÙ… Ù†Ø¨Ø§ØªÛŒ Ùˆ Ù…Ø§Ø´ÛŒÙ† Ø§Ù„Ø§Øª Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ Ùˆ Ø¯Ø§Ù…Ù¾Ø±ÙˆØ±ÛŒ Ø§Ø±Ø§ÛŒÙ‡ Ø®Ø¯Ù…Ø§Øª Ù…Ø´Ø§ÙˆØ±Ù‡ Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„ÛŒØ§Øª Ø²Ø±Ø§Ø¹ÛŒ Ùˆ Ø¨Ø§ØºØ¯Ø§Ø±ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ø§ÙØ§Øª Ø²Ø±Ø§Ø¹ÛŒ Ùˆ Ø§Ø² Ø¨ÛŒÙ† Ø¨Ø±Ø¯Ù† Ø§ÛŒÙ† Ø§ÙØ§Øª Ø¨ØµÙˆØ±Øª Ø¹Ù„Ù…ÛŒ Ø´Ø±Ú©Øª Ø¯Ø± Ù…Ù†Ø§Ù‚ØµÙ‡ Ù‡Ø§ Ùˆ Ù…Ø²Ø§ÛŒØ¯Ù‡ Ù‡Ø§ÛŒ Ø¯ÙˆÙ„ØªÛŒ Ùˆ Ø´Ø®ØµÛŒ Ø§Ø®Ø° Ù†Ù…Ø§ÛŒÙ†Ø¯Ú¯ÛŒ Ø§Ø² Ø´Ø±Ú©ØªÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ùˆ Ø®Ø§Ø±Ø¬ÛŒ Ø§Ù†Ø¬Ø§Ù… Ú©Ù„ÛŒÙ‡ Ø§Ù…ÙˆØ± Ø¨Ø§Ø²Ø±Ú¯Ø§Ù†ÛŒ Ø´Ø§Ù…Ù„ ÙˆØ§Ø±Ø¯Ø§Øª ØµØ§Ø¯Ø±Ø§Øª ØªØ±Ø®ÛŒØµ Ø­Ù‚ Ø§Ù„Ø¹Ù…Ù„ Ú©Ø§Ø±ÛŒ Ú©Ù„ÛŒÙ‡ Ú©Ø§Ù„Ø§Ù‡Ø§ÛŒ Ù…Ø¬Ø§Ø² Ø§Ø² Ú©Ù„ÛŒÙ‡ Ú¯Ù…Ø±Ú©Ø§Øª Ùˆ Ø¨Ù†Ø§Ø¯Ø± Ú©Ø´ÙˆØ± Ùˆ Ø´Ø±Ú©Øª Ø¯Ø± Ù…Ù†Ø§Ù‚ØµÙ‡ Ù‡Ø§ Ùˆ Ù…Ø²Ø§ÛŒØ¯Ù‡ Ù‡Ø§ÛŒ Ø¯ÙˆÙ„ØªÛŒ Ùˆ Ø´Ø®ØµÛŒ Ø§Ø®Ø° Ù†Ù…Ø§ÛŒÙ†Ø¯Ú¯ÛŒ Ø§Ø² Ø´Ø±Ú©ØªÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ùˆ Ø®Ø§Ø±Ø¬ÛŒ Ø§Ù†Ø¬Ø§Ù… Ú©Ù„ÛŒÙ‡ Ø¹Ù…Ù„ÛŒØ§Øª Ø§Ø¬Ø±Ø§ÛŒÛŒ Ø§Ù†ÙˆØ§Ø¹ Ø·Ø±Ø­Ù‡Ø§ÛŒ Ù…Ø®Ø§Ø¨Ø±Ø§ØªÛŒ ØªØ§Ø³ÛŒØ³Ø§ØªÛŒ Ø³Ø§Ø®ØªÙ…Ø§Ù†ÛŒ Ø±Ø§Ù‡Ø³Ø§Ø²ÛŒ Ùˆ Ú¯Ø§Ø² Ø±Ø³Ø§Ù†ÛŒ Ø§Ø¨Ø±Ø³Ø§Ù†ÛŒ Ø´Ù‡Ø±ÛŒ Ùˆ Ø±ÙˆØ³ØªØ§ÛŒÛŒ ØµÙ†Ø¹ØªÛŒ Ø´Ø§Ù…Ù„ Ø­ÙØ§Ø±ÛŒ Ú©Ø§Ø¨Ù„ Ú©Ø´ÛŒ Ù„ÙˆÙ„Ù‡ Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù†Ø§Ù„ Ø³Ø§Ø²ÛŒ Ø®Ø§Ú©Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ùˆ Ø¬Ø¯ÙˆÙ„ Ú©Ø´ÛŒ Ø§Ù†Ø¬Ø§Ù… Ú©Ù„ÛŒÙ‡ Ø§Ù…ÙˆØ± Ù¾Ø±ÙˆÚ˜Ù‡ Ù‡Ø§ÛŒ Ø³Ø§Ø®ØªÙ…Ø§Ù†ÛŒ Ù…Ø³Ú©ÙˆÙ†ÛŒ Ø§Ø³ÙØ§Ù„Øª Ú©Ø§Ø±ÛŒ Ø§Ø³Ú©Ù„Øª Ø³Ø§Ø²ÛŒ Ùˆ Ù…Ø­ÙˆØ·Ù‡ Ø³Ø§Ø²ÛŒ Ùˆ ØªØ®Ø±ÛŒØ¨ Ùˆ Ø®Ø§Ú©Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø®Ø§Ú©Ø±ÛŒØ²ÛŒ Ùˆ Ù¾ÛŒ Ú©Ù†ÛŒ Ø¯ÛŒÙˆØ§Ø± Ú†ÛŒÙ†ÛŒ Ú©Ù„ÛŒÙ‡ Ø§Ù…ÙˆØ± Ù†Ù‚Ø´Ù‡ Ú©Ø´ÛŒ (Gps â€“ GPRS) Ú©Ù„ÛŒÙ‡ Ø§Ù…ÙˆØ± Ø§Ø³Ù†Ø§Ø¯ÛŒ (Ø®Ø¯Ù…Ø§Øª Ø§Ø³Ù†Ø§Ø¯ÛŒ Ù‚Ø¨Ø¶ Ø§Ù†Ø¨Ø§Ø± Ù…Ù†Ø·Ù‚Ù‡ ÙˆÛŒÚ˜Ù‡ Ø§ØµÙ„Ø§Ø­ÛŒÙ‡ Ù…Ø§Ù†ÛŒÙØ³Øª ØµÙˆØ±ØªØ­Ø³Ø§Ø¨ Ù…Ø¬ÙˆØ² Ø®Ø±ÙˆØ¬ Ú©Ø§Ù„Ø§ (Ø¨ÛŒØ¬Ú©) ØªØ³ÙˆÛŒÙ‡ Ùˆ ØµÙˆØ±Øª Ù…Ø¬Ø§Ù„Ø³ Ø´Ù†Ø§ÙˆØ± ØµÙˆØ±Øª ÙˆØ¶Ø¹ÛŒØª Ø®Ø±ÙˆØ¬ Ø´Ù†Ø§ÙˆØ±'
        
        # Run model inference
        with st.spinner("Running inference..."):
            st.session_state.predictions = predict_sorted(st.session_state.text, tokenizer, model, THRESHOLD_DEFAULT)

            # Set the flag to True, so we know to display the results.
            st.session_state.sample_fetched = True

    # run if the button was clicked OR if a checkbox was clicked.
    if st.session_state.sample_fetched:
        # Get data from session state
        text_id = str(st.session_state.text_id)
        text = st.session_state.text
        predictions = st.session_state.predictions
        
        # Display and Styling Logic (largely the same as your code) ---
        st.markdown(
            f"""
            <link href="https://cdn.fontcdn.ir/Font/Persian/Shabnam/Shabnam.css" rel="stylesheet" type="text/css">
            <div style="text-align: justify; direction: rtl; font-family: 'Shabnam', sans-serif;">
                {text}
            </div>
            <style>
                .stCheckbox {{
                    direction: rtl;
                }}
            </style>
            """,
            unsafe_allow_html=True
        )
        
        # Post-process and display results
        if not predictions:
            st.info("No labels passed the threshold.")
            # Allow fetching a new sample if prediction fails
            if st.button("Try another sample", use_container_width=True):
                st.session_state.sample_fetched = False
                st.rerun()
            
        else:
            # Get the group details
            _, _, _, results_df = compute_hierarchy_probs(predictions, False)

            st.header("Prediction Feedback")
            
            # Get unique Sections
            sections = results_df['Section'].unique().tolist()
            for section in sections:
                st.markdown(
                    f'<h4 style="direction: rtl; font-family: \'Shabnam\', sans-serif;">ğŸ—‚ï¸ Ø¨Ø®Ø´: {section_dict[section]}</h4>',
                    unsafe_allow_html=True
                )

                # Filter DataFrame for the current section
                section_df = results_df[results_df["Section"] == section]
                divisions = section_df['Division'].unique().tolist()
                for division in divisions:
                    # We wrap everything in a div with our "rtl-container" class
                    st.markdown(
                        f'<h5 style="direction: rtl; font-family: \'Shabnam\', sans-serif;">ğŸ—ƒï¸ Ø­ÙˆØ²Ù‡: {division_dict[division]}</h5>',
                        unsafe_allow_html=True
                    )

                    # Filter DataFrame for the current division
                    division_df = section_df[section_df["Division"] == division]
                    
                    # A placeholder for all
                    col1, col2 = st.columns([11, 1])
                    with col2:
                        pass
                    with col1:
                        # Count how many columns are needed.
                        num_groups = len(division_df)
                        cols = st.columns(num_groups)

                        # Use zip to iterate through columns and data rows simultaneously.
                        for col, (_, row) in zip(cols[::-1], division_df.iterrows()):
                            with col:
                                group_name = row['Group']
                                group_prob = row['Prob']
                                
                                unique_key = f'{text_id}_{label_dict_inv[group_name]}'
                                st.checkbox("&nbsp;ØµØ­ÛŒØ­ Ø§Ø³ØªØŸ", key=unique_key)
                                
                                st.markdown(f"""
                                <div style="direction: rtl; font-family: \'Shabnam\', sans-serif;">
                                    <b>{group_dict[group_name]}</b><br>
                                    <small>Ø§Ø­ØªÙ…Ø§Ù„: {group_prob:.2f}</small>
                                </div>
                                """, unsafe_allow_html=True)
                    st.divider()
            
            st.info("Review the predictions and check the boxes for all correct labels. When you're done, press **âœ… Submit Feedback**.")
            st.markdown("---") # Visual separator

            if st.button("âœ… Submit Feedback", use_container_width=True):
                feedback_data = {
                    text_id: {
                        "full_text": text,
                        "feedback": [],
                        "labels": [label_dict_inv[p[0]] for p in predictions],
                        }
                }
                # Iterate through the results that were actually shown to the user
                for _, row in results_df.iterrows():
                    group_name = row['Group']
                    # Reconstruct the key to look it up in session_state
                    key = f"{text_id}_{label_dict_inv[group_name]}"
                    if key in st.session_state:
                        if st.session_state[key]:
                            feedback_data[text_id]["feedback"].append(label_dict_inv[group_name])
                
                # In a real app, you would save `feedback_data` to your database here.
                with st.spinner("Submitting feedback..."):
                    st.success("Feedback submitted successfully!")
                    st.write("Storing feedbacks to database")
                    st.json(feedback_data)

                # Reset the state to go back to the starting screen
                time.sleep(10)
                st.session_state.sample_fetched = False
                st.rerun()

    else:
        # This block of code runs before the button is clicked.
        st.info("Press the button above to fetch a sample text and review its prediction.")
